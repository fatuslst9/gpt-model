{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport urllib.request\nimport time\n\nimport re\nimport copy\nfrom typing import Optional, Any, Union, Callable\n\n\nimport torch\nfrom torch import nn\nfrom torch import Tensor\nfrom torch import LongTensor\n\nfrom torch.nn.init import xavier_uniform_\nfrom torch.nn.init import kaiming_uniform_\nfrom torch.nn.init import xavier_normal_\nfrom torch.utils.data import DataLoader\n\nfrom torch.nn import MultiheadAttention, ModuleList, Dropout, Linear, LayerNorm, functional as F\n\nimport torch.optim as optim\n\n\nimport math\nimport sentencepiece as spm\n\nfrom torch.optim.lr_scheduler import _LRScheduler\n\nclass CosineAnnealingWarmUpRestarts(_LRScheduler):\n    def __init__(self, optimizer, T_0, T_mult=1, eta_max=0.1, T_up=0, gamma=1., last_epoch=-1):\n        if T_0 <= 0 or not isinstance(T_0, int):\n            raise ValueError(\"Expected positive integer T_0, but got {}\".format(T_0))\n        if T_mult < 1 or not isinstance(T_mult, int):\n            raise ValueError(\"Expected integer T_mult >= 1, but got {}\".format(T_mult))\n        if T_up < 0 or not isinstance(T_up, int):\n            raise ValueError(\"Expected positive integer T_up, but got {}\".format(T_up))\n        self.T_0 = T_0\n        self.T_mult = T_mult\n        self.base_eta_max = eta_max\n        self.eta_max = eta_max\n        self.T_up = T_up\n        self.T_i = T_0\n        self.gamma = gamma\n        self.cycle = 0\n        self.T_cur = last_epoch\n        super(CosineAnnealingWarmUpRestarts, self).__init__(optimizer, last_epoch)\n    \n    def get_lr(self):\n        if self.T_cur == -1:\n            return self.base_lrs\n        elif self.T_cur < self.T_up:\n            return [(self.eta_max - base_lr)*self.T_cur / self.T_up + base_lr for base_lr in self.base_lrs]\n        else:\n            return [base_lr + (self.eta_max - base_lr) * (1 + math.cos(math.pi * (self.T_cur-self.T_up) / (self.T_i - self.T_up))) / 2\n                    for base_lr in self.base_lrs]\n\n    def step(self, epoch=None):\n        if epoch is None:\n            epoch = self.last_epoch + 1\n            self.T_cur = self.T_cur + 1\n            if self.T_cur >= self.T_i:\n                self.cycle += 1\n                self.T_cur = self.T_cur - self.T_i\n                self.T_i = (self.T_i - self.T_up) * self.T_mult + self.T_up\n        else:\n            if epoch >= self.T_0:\n                if self.T_mult == 1:\n                    self.T_cur = epoch % self.T_0\n                    self.cycle = epoch // self.T_0\n                else:\n                    n = int(math.log((epoch / self.T_0 * (self.T_mult - 1) + 1), self.T_mult))\n                    self.cycle = n\n                    self.T_cur = epoch - self.T_0 * (self.T_mult ** n - 1) / (self.T_mult - 1)\n                    self.T_i = self.T_0 * self.T_mult ** (n)\n            else:\n                self.T_i = self.T_0\n                self.T_cur = epoch\n                \n        self.eta_max = self.base_eta_max * (self.gamma**self.cycle)\n        self.last_epoch = math.floor(epoch)\n        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n            param_group['lr'] = lr\n\n\ndef preprocessing(data,max_len):\n    \n    if len(data)>max_len:\n        data[:max_len]\n            \n    else:\n        data.extend([0]*(max_len-len(data)))\n        \n    return data\n\n\ndef tokenize_and_filter(questions, answers):\n    \n    result = []\n\n    for (question, answer) in zip(questions, answers):\n        \n        sentence = question + \"[QES]\" + answer\n\n        sentence1 = sp.encode_as_ids(sentence)[:-1]\n        sentence2 = sp.encode_as_ids(sentence)[1:]\n        \n        sentence1 = preprocessing(sentence1,MAX_LENGTH)\n        sentence2 = preprocessing(sentence2,MAX_LENGTH)\n       \n        result.append([sentence1,sentence2])\n       \n    return result\n\n\n\ndef collate_fn(batch):\n    \n    train_iter = torch.Tensor(np.array(batch)).long()\n    \n    data = train_iter.transpose(0,1)\n    \n    return data[0], data[1]\n    \n    \n    \nMAX_LENGTH = 50\n\nvocab_size=2**13\n\nurllib.request.urlretrieve(\"https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv\", filename=\"ChatBotData.csv\")\ntrain_data = pd.read_csv('ChatBotData.csv')\ntrain_data.head()\n\nquestions = []\nfor sentence in train_data['Q']:\n \n    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n    sentence = sentence.strip()\n    questions.append(sentence)\n    \nanswers = []\nfor sentence in train_data['A']:\n    \n    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n    sentence = sentence.strip()\n    answers.append(sentence)\n    \nprint(f\"데이터 샘플\")\nprint(f\"질문: {questions[0]}\")\nprint(f\"대답: {answers[0]}\")\n    \nsentences = questions+answers\n    \nwith open('subword_train.txt','w',encoding='utf-8') as f:\n    for line in sentences:\n        f.write(line + '\\n')\n        \nwith open('subword_train.txt','r',encoding='utf-8') as f:\n    test = f.read().split('\\n')\n\n\ninput_file = 'subword_train.txt'\n\nmodel_name='subword_tokenizer_kor'\n\nmodel_type = 'bpe'\n\nuser_defined_symbols = \"[QES]\"\n\ninput_argument = '--input=%s --model_prefix=%s --vocab_size=%s --model_type=%s --user_defined_symbols=%s'\n\ncmd = input_argument%(input_file, model_name, vocab_size, model_type, user_defined_symbols)\n\nspm.SentencePieceTrainer.Train(cmd)\n\nsp = spm.SentencePieceProcessor()\n\nsp.Load('subword_tokenizer_kor.model')\n\nsp.SetEncodeExtraOptions('bos:eos')\n\n\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nd_hid = 512\nnlayers = 2\nnhead = 8\ndropout = 0.1\nd_model = 256\nemb_size = d_model\nclass_num = 3\nbatch_size=64  \n\nPAD_IDX, BOS_IDX, EOS_IDX, QES_IDX = 0, 1, 2, 3\n\ntrain_iter = tokenize_and_filter(questions,answers)\n\ndataloader = DataLoader(train_iter,batch_size=batch_size,shuffle=True,collate_fn=collate_fn)\n\n\n\nclass GPT(nn.Module):\n    \n    def __init__(self, d_model: int = 512, nhead: int = 8, num_decoder_layers: int = 6, dim_feedforward: int = 2048,\n                 dropout: float = 0.1, activation: Union[str, Callable[[Tensor], Tensor]] = F.relu, \n                 custom_decoder: Optional[Any] = None, layer_norm_eps: float = 1e-5, \n                 norm_first: bool = False) -> None:\n        \n        super(GPT, self).__init__()\n        \n        self.tok_emb = TokenEmbedding(vocab_size, emb_size) \n        self.positional_encoding = PositionalEncoding(emb_size, dropout=dropout)    \n\n\n        if custom_decoder is not None:\n            self.decoder = custom_decoder\n        else:\n            decoder_layer = TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout,\n                                                    activation, layer_norm_eps, norm_first)\n            decoder_norm = LayerNorm(d_model, eps=layer_norm_eps)\n            self.decoder = TransformerDecoder(decoder_layer, num_decoder_layers, decoder_norm)\n            \n        self.linear_lm =  nn.Linear(emb_size, vocab_size)\n        self.linear_cls = nn.Linear(emb_size, class_num)\n            \n        self._reset_parameters()\n\n        self.d_model = d_model\n        self.nhead = nhead\n\n        \n\n    def forward(self, tgt: Tensor, tgt_mask: Optional[Tensor] = None, tgt_key_padding_mask: Optional[Tensor] = None) -> [Tensor]:\n        output = self.positional_encoding(self.tok_emb(tgt))\n        output = self.decoder(output, tgt_mask=tgt_mask, tgt_key_padding_mask=tgt_key_padding_mask)\n        output1 = self.linear_lm(output)\n        output2 = self.linear_cls(output)\n        return output1, output2\n\n    \n\n    def _reset_parameters(self):\n        r\"\"\"Initiate parameters in the transformer model.\"\"\"\n\n        for p in self.parameters():\n            if p.dim() > 1:\n                xavier_normal_(p)\n                \n                \n    def decode(self, tgt: Tensor, tgt_mask: Tensor):\n        return self.decoder(self.positional_encoding(self.tok_emb(tgt)),tgt_mask)\n                \n                \nclass TransformerDecoder(nn.Module):\n    \n    __constants__ = ['norm']\n\n    def __init__(self, decoder_layer, num_layers, norm=None):\n        super(TransformerDecoder, self).__init__()\n        self.layers = _get_clones(decoder_layer, num_layers)\n        self.num_layers = num_layers\n        self.norm = norm\n\n    def forward(self, tgt: Tensor, tgt_mask: Optional[Tensor] = None, tgt_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n        \n        output = tgt\n\n        for mod in self.layers:\n            output = mod(output, tgt_mask=tgt_mask, tgt_key_padding_mask=tgt_key_padding_mask )\n\n        if self.norm is not None:\n            output = self.norm(output)\n\n        return output\n    \n    \n    \nclass TransformerDecoderLayer(nn.Module):\n    \n    \n    __constants__ = ['norm_first']\n\n    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=F.relu,\n                 layer_norm_eps=1e-5, norm_first=False) -> None:\n      \n        \n        super(TransformerDecoderLayer, self).__init__()\n        \n        self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout)\n        self.multihead_attn = MultiheadAttention(d_model, nhead, dropout=dropout)\n        \n        self.linear1 = Linear(d_model, dim_feedforward)\n        self.dropout = Dropout(dropout)\n        self.linear2 = Linear(dim_feedforward, d_model)\n\n        self.norm_first = norm_first\n        self.norm1 = LayerNorm(d_model, eps=layer_norm_eps)\n        self.norm2 = LayerNorm(d_model, eps=layer_norm_eps)\n        \n        self.dropout1 = Dropout(dropout)\n        self.dropout2 = Dropout(dropout)\n       \n        if isinstance(activation, str):\n            self.activation = _get_activation_fn(activation)\n        else:\n            self.activation = activation\n\n    def __setstate__(self, state):\n        if 'activation' not in state:\n            state['activation'] = F.relu\n        super(TransformerDecoderLayer, self).__setstate__(state)\n\n    def forward(self, tgt: Tensor, tgt_mask: Optional[Tensor] = None, tgt_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n       \n        x = tgt\n        if self.norm_first:\n            x = x + self._sa_block(self.norm1(x), tgt_mask, tgt_key_padding_mask)\n            x = x + self._ff_block(self.norm2(x))\n        else:\n            x = self.norm1(x + self._sa_block(x, tgt_mask, tgt_key_padding_mask))\n            x = self.norm2(x + self._ff_block(x))\n\n        return x\n\n    # self-attention block\n    def _sa_block(self, x: Tensor, attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor]) -> Tensor:\n        x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0] \n        return self.dropout1(x)\n\n\n    # feed forward block\n    def _ff_block(self, x: Tensor) -> Tensor: \n        x = self.linear2(self.dropout(self.activation(self.linear1(x))))\n        return self.dropout2(x)\n\n\n\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self,\n                 emb_size: int,\n                 dropout: float,\n                 maxlen: int = 5000):\n        super(PositionalEncoding, self).__init__()\n        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n        pos_embedding = torch.zeros((maxlen, emb_size))\n        pos_embedding[:, 0::2] = torch.sin(pos * den)\n        pos_embedding[:, 1::2] = torch.cos(pos * den)\n        pos_embedding = pos_embedding.unsqueeze(-2)\n\n        self.dropout = nn.Dropout(dropout)\n        self.register_buffer('pos_embedding', pos_embedding)\n\n    def forward(self, token_embedding: Tensor):\n        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n\n\nclass TokenEmbedding(nn.Module):\n    def __init__(self, vocab_size: int, emb_size):\n        super(TokenEmbedding, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, emb_size)\n        self.emb_size = emb_size\n\n    def forward(self, tokens: Tensor):\n        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n\n    \n\nclass CustomSchedule():\n\n    def __init__(self, d_model, warmup_steps=4000):\n        self.d_model = d_model\n        self.warmup_steps = warmup_steps\n        \n    def call(self,epoch):\n        arg1 = 1/math.sqrt(epoch)\n        arg2 = epoch * (self.warmup_steps**-1.5)\n\n        return 1/math.sqrt(self.d_model) * min(arg1, arg2)\n    \n    \ndef _get_clones(module, N):\n    return ModuleList([copy.deepcopy(module) for i in range(N)])\n\n\ndef _get_activation_fn(activation):\n    if activation == \"relu\":\n        return F.relu\n    elif activation == \"gelu\":\n        return F.gelu\n\n    raise RuntimeError(\"activation should be relu/gelu, not {}\".format(activation))        \n    \ndef generate_square_subsequent_mask(sz):\n    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n    return mask\n\n\ndef create_mask(tgt):\n    tgt_seq_len = tgt.shape[0]\n    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n    return tgt_mask, tgt_padding_mask\n\n\n    \ndef greedy_decode(model, ys, max_len):\n    \n    ys = ys.to(DEVICE)\n    for i in range(max_len-1):\n        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n                    .type(torch.bool)).to(DEVICE)\n        out = model.decode(ys, tgt_mask)\n        out = out.transpose(0, 1)\n        prob = model.linear_lm(out[:, -1])\n        _, next_word = torch.max(prob, dim=1)\n        next_word = next_word.item()\n        ys = torch.cat([ys, torch.ones(1, 1).type_as(ys.data).fill_(next_word)], dim=0).to(DEVICE)\n        if next_word == EOS_IDX:\n            break\n    return ys\n\n\n\ndef reply(model: torch.nn.Module, src_sentence: str):\n    \n    model.eval()\n    token = sp.encode_as_ids(src_sentence)\n    token.append(QES_IDX)\n    ys = torch.Tensor(token).view(-1, 1)\n    tgt_tokens = greedy_decode(model,  ys, max_len=vocab_size + 5).flatten().to(DEVICE)\n    answer = \" \".join(sp.decode(tgt_tokens.long().tolist()[len(token):]))\n    return answer\n\n\ngpt = GPT(d_model = d_model, nhead = nhead, num_decoder_layers = nlayers , dim_feedforward = d_hid, dropout = dropout)\ngpt = gpt.to(DEVICE)\nloss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\noptimizer = optim.Adam(gpt.parameters(), lr = 0)\nscheduler = CosineAnnealingWarmUpRestarts(optimizer, T_0=30, T_mult=1, eta_max=0.008,  T_up=10, gamma=0.8)\n\ndef train(model: nn.Module, epoch):\n    \n    model.train()\n    losses = 0 \n    batch=1\n\n    for targets1, targets2 in dataloader:        \n        targets1= targets1.transpose(0,1).to(DEVICE)\n        targets2= targets2.transpose(0,1).to(DEVICE)\n        tgt_mask, tgt_padding_mask = create_mask(targets1)\n        output1, output2 = model(tgt=targets1, tgt_mask = tgt_mask, tgt_key_padding_mask = tgt_padding_mask)\n        optimizer.zero_grad()\n        loss = loss_fn(output1.reshape(-1,output1.shape[-1]), targets2.reshape(-1))\n        loss.backward()\n        losses += loss.item()\n        optimizer.step()\n    scheduler.step()\n    return losses/len(dataloader)\n\n\nepochs = 300\nfor epoch in range(1,epochs+1):\n    loss = train(gpt,epoch)\n    if epoch%30==0 or epoch==1:\n        \n        print(\"epoch :\",epoch , \"loss: \" ,loss)\n        try:\n            print(reply(gpt, \"배고프다\"))\n        except:\n            continue\n            \n        ","metadata":{"execution":{"iopub.status.busy":"2022-02-10T08:19:30.171309Z","iopub.execute_input":"2022-02-10T08:19:30.171678Z","iopub.status.idle":"2022-02-10T08:41:34.868425Z","shell.execute_reply.started":"2022-02-10T08:19:30.171645Z","shell.execute_reply":"2022-02-10T08:41:34.867521Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}